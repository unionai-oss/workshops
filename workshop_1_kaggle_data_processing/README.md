# Workshop: Getting Data from Kaggle Workshop

This workshop will show you how to get data from Kaggle and process it for use in your projects.

In order to get the data from Kaggle, we must use a command line tool. To make this possible we need a Flyte task that runs a bash script instead of a Python function. We can achieve this by using a `ContainerTask`. This lets us define a custom container image that includes the desired bash script. The container for this task will be defined by a standard Dockerfile and will be built and pushed to a container registry before registering and running the Flyte workflow.

In addition, we will use a normal `PythonFunctionTask` (that is, an `@task` decorated Python function) to perform deduplication on the Kaggle data. The container for this task will be defined using an `ImageSpec` declaration in the `@task` decorator. The image will be built and pushed to the container registry as part of the Flyte workflow registration step.


## Features

The following advanced Flyte features will be covered:
- Raw ContainerTasks
- AWS Secrets Manager integration
- Imagespec
- Integration Testing
- CI/CD


## Prerequisites

* [Docker](https://docs.docker.com/get-docker/)
* [Flyte](https://docs.flyte.org/en/latest/getting_started/installation.html)
* [ghcr.io](https://github.com/features/packages) or other image registry access. Make sure you have logged into the registry through docker on your local machine.
* An AWS-based Flyte installation (GCP and Azure versions of this workshop are forthcoming).


## Setup

### Clone

Clone this repository.


### Ensure access to container registry

Through docker, login to your container registry on your local machine. For example, for `ghcr.io` you would do `docker login gchr.io`

### Set yourself up on Kaggle

You will need to set yourself up on `kaggle.com`.

* Create a [Kaggle Account](https://www.kaggle.com/).

* Create a [Kaggle API Token](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication).


### Add your API token to AWS Secret Manager

Add a secret to you AWS Secret Manager containing your Kaggle API token, by following [this guide](https://docs.union.ai/integrations/enabling-aws-resources/enabling-aws-secrets-manager). 

* The API token will be a JSON string something like this

  `{"username":"my_user_name","key":"11b3e5cc667ecbe580d54a3a1d280de3"}`.

* Add the entire JSON object to Secret Manager as **Other type of secret > Plaintext** and give the secret a name.

* Secret Manager will present an ARN that looks like this:
        
  `arn:aws:secretsmanager:<your-region>:<your-account-id>:secret:<your-secret-name>-<autogenerated-suffix>`.

* Use the ARN to define the `SECRET_GROUP` and `SECRET_KEY` constants in `config.py`, as described below.


### Define all your config parameters

In `config.py` fill in the all the parameters:

* `registry_prefix`: The URL prefix of your registry, not including the image name. For example, if you are using `ghcr.io`, this will be

  `ghcr.io/<your-github-username>`.

* `kaggle_task_image_name_and_tag`:  The name and tag of the image used by the `ContainerTask` that gets the Kaggle dataset.
  
  This should be a full name plus tag. For example, `kaggle-task-image:latest`.

* `dedupe_task_image_name`: This image is used by `dedupe_task`, and is referenced in the `ImageSpec`.

  This should be just the image name without a tag (the `ImageSpec` builder will provide the tag). For example, `dedupe-task-image`.

* `SECRET_GROUP` and `SECRET_KEY`: You should have a secret ARN that looks like this: 

  `arn:aws:secretsmanager:<your-region>:<your-account-id>:secret:<your-secret-name>-<autogenerated-suffix>`. 

  You need to divide it up into two strings.

  * `SECRET_GROUP`: The part of the ARN up to and including `:secret:`, in this case, 
    
    `arn:aws:secretsmanager:<your-region>:<your-account-id>:secret:`.
  
  * `SECRET_KEY`: The part of the ARN after `:secret:`. In this case,
  
    `<your-secret-name>-<autogenerated-suffix>`.
  
* `target_project`: The name of the project where you registered the workflow.

* `target_domain`: The name of the domain where you registered the workflow


### Build the image to be used for the ContainerTask

Substituting the values specified for `registry_prefix` and `kaggle_task_image_name_and_tag`, above, run the following commands:

* `docker build --platform linux/amd64 -f Dockerfile -t {registry_prefix}/{kaggle_task_image_name_and_tag} .` (don't forget the dot at the end!).

* `docker push {registry_prefix}/{kaggle_task_image_name_and_tag}`.


### Install all your dependencies locally

Install your dependencies locally:

* Run `pip install -r requirements.txt`


### Register your workflows and tasks

Make sure your `FLYTECTL_CONFIG` environment variable is pointing to a `config.yaml` file specifying your target Flyte installation.
Then, substituting the values specified for `target_project` and `target_domain` above:

* Run `pyflyte register kaggle_data_processing --project {target_project} --domain {target_domain}`


### Ensure images are public

On successful registration, the `ImageSpec` builder will build and push the image `{registry_prefix}/{kaggle_task_image_name_and_tag}`. Earlier you manually built and pushed the image `{registry_prefix}/{kaggle_task_image_name_and_tag}`. 

You must now ensure that both of these images are publicly accessible so that Union Cloud can pull the images when it runs the workflow. The details on how to do this depend on your registry.


### Run the workflow in Union Cloud though the UI

* In the Flyte console go to `{target_domian}` in `{target_project}`, find this workflow (it should be called `kaggle_data_processing.workflows.deduplication.deduplication_wf`) and click *Launch Workflow*.

* In the dialog that appears, specify the inputs using the same values as found in `tests/cases.py`, either

  * `dataset_name`: `joelljungstrom/128k-airline-reviews` and `file_name`: `AirlineReviews.csv` or,
  
  * `dataset_name`: `ankitkumar2635/sentiment-and-emotions-of-tweets` and `file_name`: `sentiment-emotion-labelled_Dell_tweets.csv`.


### Run the workflow programmatically with FlyteRemote

The test suite uses `FlyteRemote` to run the workflow on the Flyte installation through a Python program running on your local machine:

* Run `pytest` and wait for the tests to complete.

(Note that the test code depends upon your `FLYECTL_CONFIG` environment variable pointing to a `config.yaml` specifying your target Flyte installation)
