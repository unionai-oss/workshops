{
    "model_path": "EleutherAI/pythia-1b",
    "output_dir": "./output",
    "checkpoint_dir": null,
    "num_epochs": 1,
    "batch_size": 16,
    "test_size": 0.001,
    "model_max_length": 512,
    "seed": 41,
    "report_to": "wandb",
    "device_map": "auto",
    "gradient_accumulation_steps": 8,
    "padding": "left",
    "dataloader_num_proc": 8,
    "use_fp16": true,
    "use_4bit": true,
    "use_qlora": true,
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_target_modules": ["dense_h_to_4h", "dense_4h_to_h"],
    "lora_dropout": 0.05,
    "debug": false,
    "publish_config": {
        "repo_id": "Union-AI-OSS/flyte-pythia-1b-lora-adapter-v0",
        "readme": "# flyte-pythia-70m fine-tuned on Flyte repos",
        "language": "python",
        "model_card": {
            "language": ["en"],
            "license": "apache-2.0",
            "tags": [
                "pytorch",
                "causal-lm",
                "pythia",
                "fine-tuning",
                "flyte repo dataset"
            ]
        }
    }
}
