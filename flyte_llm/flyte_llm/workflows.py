"""Flyte LLama workflows."""

import os
from pathlib import Path
from typing import List, Optional, Union

from flytekit import task, workflow, current_context, Resources, Secret, ImageSpec
from flytekit.loggers import logger
from flytekit.types.directory import FlyteDirectory

from flyte_llm.dataset import create_dataset_fn, REPO_URLS
from flyte_llm.train import train_fn, TrainerConfig
from flyte_llm.inference import infer, load_pipeline
from flyte_llm.publish import publish_to_hf_hub

# Union Managed Secrets
# Configured for TMLS use only
SECRET_GROUP = "arn:aws:secretsmanager:us-east-2:356633062068:secret:"
WANDB_API_SECRET_KEY = "wand_api_key_tmls-PLs22C"
HF_HUB_API_SECRET_KEY = "huggingface_hub_api_key_tmls-CB6DVk"

# You shouldn't need to update this, any update will require a local docker build / push which will
# Consume a substantial amount of bandwidth.
image_spec = ImageSpec(
    name="flyte-llama-qlora",
    apt_packages=["git"],
    registry="ghcr.io/unionai-oss",
    requirements="requirements.txt",
    python_version="3.9",
    cuda="11.7.1",
    env={"VENV": "/opt/venv"},
)


@task(
    cache=True,
    cache_version="3",
    container_image=image_spec,
    requests=Resources(mem="8Gi", cpu="2", ephemeral_storage="8Gi"),
)
def create_dataset(additional_github_repo_url: Optional[str] = None) -> FlyteDirectory:
    urls = [*REPO_URLS, *(additional_github_repo_url or [])]

    ctx = current_context()
    working_dir = Path(ctx.working_directory)
    output_dir = working_dir / "dataset"
    repo_cache_dir = working_dir / "repo_cache"

    create_dataset_fn(urls, output_dir, repo_cache_dir)
    return FlyteDirectory(path=str(output_dir))


@task(
    container_image=image_spec,
    requests=Resources(mem="10Gi", cpu="10", gpu="4", ephemeral_storage="100Gi"),
    environment={
        "WANDB_PROJECT": "qlora-llama2-fine-tuning-tmls",
        "TRANSFORMERS_CACHE": "/tmp",
        "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION": "python",
    },
    secret_requests=[
        Secret(
            group=SECRET_GROUP,
            key=WANDB_API_SECRET_KEY,
            mount_requirement=Secret.MountType.FILE,
        ),
        Secret(
            group=SECRET_GROUP,
            key=HF_HUB_API_SECRET_KEY,
            mount_requirement=Secret.MountType.FILE,
        ),
    ],
)
def train_task(
    dataset: FlyteDirectory,
    config: TrainerConfig,
) -> FlyteDirectory:
    """
    This task is used to train a model using the Flyte LLM framework.
    :param dataset:  A FlyteDirectory containing the dataset to be used for training, generated by the create_dataset task
    :param config:  A TrainerConfig object containing the parameters for training, use the pre-defined defaults or create your own
    :return:
    """
    if int(os.environ.get("LOCAL_RANK", 0)) == 0:
        logger.info(f"Training Flyte Llama with params:\n{config}")
    wandb_run_name = os.environ.get("FLYTE_INTERNAL_EXECUTION_ID", "local")
    os.environ["WANDB_RUN_ID"] = wandb_run_name

    ctx = current_context()
    try:
        os.environ["WANDB_API_KEY"] = ctx.secrets.get(SECRET_GROUP, WANDB_API_SECRET_KEY)
    except ValueError:
        pass

    dataset.download()
    config.data_dir = dataset.path

    try:
        hf_auth_token = ctx.secrets.get(SECRET_GROUP, HF_HUB_API_SECRET_KEY)
    except ValueError:
        hf_auth_token = None

    train_fn(config, hf_auth_token)
    return FlyteDirectory(path=str(config.output_dir))

@task(
    container_image=image_spec,
    requests=Resources(mem="10Gi", cpu="10", gpu="4", ephemeral_storage="100Gi"),
    environment={
        "WANDB_PROJECT": "qlora-llama2-fine-tuning-tmls",
        "TRANSFORMERS_CACHE": "/tmp",
        "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION": "python",
    },
    secret_requests=[
        Secret(
            group=SECRET_GROUP,
            key=WANDB_API_SECRET_KEY,
            mount_requirement=Secret.MountType.FILE,
        ),
        Secret(
            group=SECRET_GROUP,
            key=HF_HUB_API_SECRET_KEY,
            mount_requirement=Secret.MountType.FILE,
        ),
    ],
)
def inference_lora(prompt: str, adapter_path: Union[FlyteDirectory, str], config: TrainerConfig) -> dict:
    """
    This inference task can be used to run inference on a trained model.
    The model can either be the direct s3 output from the train_workflow stage, or a hugging face model hub path.
    :param prompt: List of strings to be used as the prompt for the model
    :param adapter_path: Either a FlyteDirectory or a string path to a hugging face model hub
    :param config: The TrainerConfig used to create the Adapter; contains the original model
    :return:
    """
    if isinstance(adapter_path, FlyteDirectory):
        adapter_path.download()
        adapter_path = adapter_path.path
    else:
        adapter_path = adapter_path
    model = load_pipeline(config, adapter_path)
    return infer(model, prompt)

@task(
    retries=3,
    cache=True,
    cache_version="0.0.4",
    container_image=image_spec,
    requests=Resources(mem="10Gi", cpu="1", ephemeral_storage="64Gi"),
    secret_requests=[
        Secret(
            group=SECRET_GROUP,
            key=HF_HUB_API_SECRET_KEY,
            mount_requirement=Secret.MountType.FILE,
        ),
    ],
)
def publish_adapter(
    adapter_dir: FlyteDirectory,
    adapter_name: str,
    config: TrainerConfig,
) -> str:
    adapter_dir.download()
    adapter_dir = Path(adapter_dir.path)
    ctx = current_context()

    try:
        hf_auth_token = ctx.secrets.get(SECRET_GROUP, HF_HUB_API_SECRET_KEY)
    except Exception:
        hf_auth_token = None

    return publish_to_hf_hub(adapter_dir, adapter_name, config, hf_auth_token)

"""
WORKFLOWS
Workflows are the top level abstraction in Flyte. They are composed of tasks and other workflows.
"""
@workflow
def train_workflow(
    config: TrainerConfig,
    custom_github_url: Optional[str] = None,
) -> FlyteDirectory:
    dataset = create_dataset(additional_github_repo_url=custom_github_url)
    model = train_task(
        dataset=dataset,
        config=config,
    )
    return model



@workflow
def publish_adapter_workflow(
    adapter_dir: FlyteDirectory,
    adapter_name: str,
    config: TrainerConfig,
) -> str:
    return publish_adapter(adapter_dir=adapter_dir, adapter_name=adapter_name, config=config)

@workflow
def inference_workflow(
    prompt: str,
    adapter_path: Union[FlyteDirectory, str],
    config: TrainerConfig,
) -> dict:
    return inference_lora(prompt=prompt, adapter_path=adapter_path, config=config)